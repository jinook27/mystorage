{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() \n",
    "                      else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def function_2d(x,y):\n",
    "    ### shape ### x,y:(num_ins,1) \n",
    "    \n",
    "    term_sqrt = np.sqrt(x*x+y*y)\n",
    "    term1 = np.sin(20*term_sqrt)/(20*term_sqrt)\n",
    "    term2 = (1/5)*np.cos(10*term_sqrt)\n",
    "    term3 = y/2 - 0.3\n",
    "    \n",
    "    label = term1 + term2 + term3\n",
    "    \n",
    "    return label\n",
    "    ###shape### label : (num_ins,1)\n",
    "\n",
    "# generate dataset\n",
    "def generate_data(dim):\n",
    "    \n",
    "    #generate 100x100 points\n",
    "    x = np.linspace(-1,1,dim)\n",
    "    y = np.linspace(-1,1,dim)\n",
    "    xx,yy = np.meshgrid(x,y)\n",
    "    ###shape### xx,yy : (dim,dim)\n",
    "    zz = function_2d(xx,yy)\n",
    "    ###shape### zz : (dim,dim)\n",
    "    \n",
    "    # add noise\n",
    "    noise = np.random.standard_normal((dim,dim))\n",
    "    zz = zz + 0.1*noise\n",
    "    \n",
    "    # create dataset\n",
    "    data1 = xx.reshape(-1,1)\n",
    "    data2 = yy.reshape(-1,1)\n",
    "    label = zz.reshape(-1,1)\n",
    "    dataset = np.hstack((data1,data2))\n",
    "    dataset = np.hstack(((dataset,label)))\n",
    "    ###shape### dataset : (dim*dim,3) np.array\n",
    "    return torch.Tensor(dataset)\n",
    "\n",
    "dataset= generate_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., -1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainset, validset = train_test_split(dataset, test_size=0.3, random_state=0)\n",
    "validset, testset = train_test_split(validset,test_size=0.5,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "#ngpu = 1\n",
    "#hidden_neurons = 16\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_neurons):\n",
    "        super(MLP, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.Linear(2,hidden_neurons),\n",
    "            #nn.ReLU(True),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_neurons,1),\n",
    "            \n",
    "           # nn.Dropout(0.2),\n",
    "           # nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,2)\n",
    "        return self.main(x)\n",
    "    \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(0.0,1.0)\n",
    "        torch.nn.init.zeros_(m.bias.data)\n",
    "        \n",
    "#model=MLP(ngpu)#.to(device)\n",
    "#model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Generator's state_dict:\\n\")\n",
    "# for param_tensor in model.state_dict():\n",
    "#     print(param_tensor, \"\\t \", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model, (1,1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-23d116042a9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcriterian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "def rmse(output ,label):\n",
    "    sum0 = 0\n",
    "    for i in range(0,1500):\n",
    "        \n",
    "        term1 = float(output[i])-float(label[i])\n",
    "        term2 = term1 * term1\n",
    "        sum0 = sum0 + term2\n",
    "        \n",
    "    result = np.sqrt(sum0/1500)\n",
    "    #result = torch.Tensor(result)\n",
    "    result = torch.from_numpy(np.array(result))\n",
    "    result = Variable(result,requires_grad=True)\n",
    "    return result\n",
    "   # return torch.from_numpy(result)\n",
    "    \n",
    "criterian = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3000, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Train Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    #print('epoch : ',epoch+1)\n",
    "    model.train() \n",
    "\n",
    "    inputs = Variable(trainset[:,0:2]).view(7000, 2)#.to(device)\n",
    "    labels = Variable(trainset[:,2]).view(7000, 1)#.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "   # train_out.append(outputs)\n",
    "\n",
    "    loss = criterian(outputs,labels)\n",
    "#     if(epoch%5000==0 or epoch==num_epochs-1):\n",
    "#         print(epoch,loss.item())\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "def valid(epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        inputs = Variable(validset[:,0:2]).view(1500, 2)#.to(device)\n",
    "        labels = Variable(validset[:,2]).view(1500, 1)#.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = rmse(outputs,labels)\n",
    "\n",
    "        if(epoch%3000==0 or epoch==num_epochs-1):\n",
    "            print(epoch,'valid: Loss: {:.4f}'.format(loss.item()))      \n",
    "    #scheduler.step()      \n",
    "    return loss.item()\n",
    "    \n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        inputs = Variable(testset[:,0:2]).view(1500, 2)#.to(device)\n",
    "        labels = Variable(testset[:,2]).view(1500, 1)#.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = rmse(outputs,labels)\n",
    "\n",
    "        if(epoch%3000==0 or epoch==num_epochs-1):\n",
    "            print(epoch,'test: Loss: {:.4f}'.format(loss.item()))\n",
    "    #scheduler.step()    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 10000\n",
    "Err_val=[]  \n",
    "h=[16,24,32,40,48,56]\n",
    "dataset= generate_data(100)\n",
    "for fold in range(0,3):\n",
    "    dataset_shuffled=dataset[torch.randperm(len(dataset))]\n",
    "    valid_n_test=dataset_shuffled[3000*fold:3000*(fold+1)][:]\n",
    "    validset = valid_n_test[0:1500][:]\n",
    "    \n",
    "    if(fold == 0):\n",
    "        trainset = dataset_shuffled[3000:10000][:]\n",
    "    elif(fold == 1):\n",
    "        t1 = dataset_shuffled[0:3000][:]\n",
    "        t2 = dataset_shuffled[6000:10000][:]\n",
    "        t = np.vstack((t1,t2))\n",
    "        trainset = torch.Tensor(t)\n",
    "    else:\n",
    "        t1 = dataset_shuffled[0:6000][:]\n",
    "        t2 = dataset_shuffled[9000:10000][:]\n",
    "        t = np.vstack((t1,t2))\n",
    "        trainset = torch.Tensor(t) \n",
    "    \n",
    "    for i in range(0,len(h)):\n",
    "        print('fold =',fold,', hidden neurons =',h[i])\n",
    "        model=MLP(h[i])\n",
    "        model.apply(weights_init)\n",
    "        for epoch in range(0,num_epochs):\n",
    "            train(epoch)\n",
    "            a = valid(epoch)\n",
    "        Err_val.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Err_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
