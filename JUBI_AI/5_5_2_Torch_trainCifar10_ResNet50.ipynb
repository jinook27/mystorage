{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8fvk1GS7KmO"
   },
   "source": [
    "##1. Upload and visualize CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szcm5SQX7R-O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.utils as utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as skl\n",
    "\n",
    "\n",
    "\n",
    "# Define CNN Architecture\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 150\n",
    "\n",
    "\n",
    "#Visualizing CIFAR 10\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data',download=True)\n",
    "datanum = len(trainset)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "fig = plt.figure()\n",
    "ims = np.random.randint(datanum, size=15)\n",
    "\n",
    "for i in range(15):\n",
    "    subplot = fig.add_subplot(3,5, i+1)\n",
    "    subplot.set_xticks([])\n",
    "    subplot.set_yticks([])\n",
    "    PILimg, label = trainset[ims[i]]\n",
    "    subplot.set_title(\"%s\" %classes[label])\n",
    "    subplot.imshow(PILimg)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JXoLWO58JgA"
   },
   "source": [
    "## 2. Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsMO162S8kVu"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPNopboF8uoI"
   },
   "source": [
    "## 3. Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhtVibcI9PsE"
   },
   "outputs": [],
   "source": [
    "# Model \n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet50, self).__init__()\n",
    "\n",
    "        self.conv1 = self.conv1_layer()\n",
    "        self.conv2 = self.conv2_layer()\n",
    "        self.conv3 = self.conv3_layer()\n",
    "        self.conv4 = self.conv4_layer()\n",
    "        self.conv5 = self.conv5_layer()\n",
    "        self.linear = nn.Linear(2048, num_classes)\n",
    "\n",
    "\n",
    "    def conv1_layer(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def conv2_layer(self):\n",
    "        layers = []\n",
    "        layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        layers.append(conv_block(in_channels=64, filters = [64,64,256], strides=1))\n",
    "        layers.append(identity_block(filters = [64,64,256]))\n",
    "        layers.append(identity_block(filters = [64,64,256]))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def conv3_layer(self):\n",
    "        layers = []\n",
    "        layers.append(conv_block(in_channels=256, filters = [128,128,512], strides=2))\n",
    "        layers.append(identity_block(filters = [128,128,512]))\n",
    "        layers.append(identity_block(filters = [128,128,512]))\n",
    "        layers.append(identity_block(filters = [128,128,512]))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def conv4_layer(self):\n",
    "        layers = []\n",
    "        layers.append(conv_block(in_channels=512, filters = [256,256,1024], strides=2))\n",
    "        layers.append(identity_block(filters = [256,256,1024]))\n",
    "        layers.append(identity_block(filters = [256,256,1024]))\n",
    "        layers.append(identity_block(filters = [256,256,1024]))\n",
    "        layers.append(identity_block(filters = [256,256,1024]))\n",
    "        layers.append(identity_block(filters = [256,256,1024]))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def conv5_layer(self):\n",
    "        layers = []\n",
    "        layers.append(conv_block(in_channels=1024,filters = [512,512,2048], strides=2))\n",
    "        layers.append(identity_block(filters = [512,512,2048]))\n",
    "        layers.append(identity_block(filters = [512,512,2048]))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.conv5(out)\n",
    "        out = F.avg_pool2d(out, kernel_size=1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, filters, strides):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.filters1, self.filters2, self.filters3 = filters\n",
    "        self.conv1 = nn.Conv2d(in_channels, self.filters1, kernel_size=1, stride=strides, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.filters1)\n",
    "        self.conv2 = nn.Conv2d(self.filters1, self.filters2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.filters2)\n",
    "        self.conv3 = nn.Conv2d(self.filters2, self.filters3, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.filters3)\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.filters3, kernel_size=1, stride=strides, bias=False),\n",
    "            nn.BatchNorm2d(self.filters3)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        #out = F.relu(out)\n",
    "        out = nn.ReLU(inplace=True)(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class identity_block(nn.Module):\n",
    "\n",
    "    def __init__(self, filters):\n",
    "        super(identity_block, self).__init__()\n",
    "        self.filters1, self.filters2, self.filters3 = filters\n",
    "        self.conv1 = nn.Conv2d(self.filters3, self.filters1, kernel_size=1, stride=1, padding=0, bias=False)#padding=0=valid\n",
    "        self.bn1 = nn.BatchNorm2d(self.filters1)\n",
    "        self.conv2 = nn.Conv2d(self.filters1, self.filters2, kernel_size=3, stride=1, padding=1, bias=False)#padding=1=same\n",
    "        self.bn2 = nn.BatchNorm2d(self.filters2)\n",
    "        self.conv3 = nn.Conv2d(self.filters2, self.filters3, kernel_size=1, stride=1, padding=0, bias=False)#padding=0=valid\n",
    "        self.bn3 = nn.BatchNorm2d(self.filters3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out += x\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "print('==> Building model..')\n",
    "net = ResNet50()\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0ct_m-P9gL-"
   },
   "source": [
    "## 4. Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z25zyY4h9aMh"
   },
   "outputs": [],
   "source": [
    "#**************************************************\n",
    "#                Training and testing\n",
    "#**************************************************\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    print('\\nTrain:')\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print_time = -1\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if batch_idx+1 == len(trainloader):\n",
    "            print('[%3d/%3d] | Loss: %.3f | Acc: %.3f%% (%d/%d)'%(\n",
    "                batch_idx+1, len(trainloader), train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    print('\\nTest:')\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print_time = -1\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if batch_idx+1 == len(testloader):\n",
    "                print('[%3d/%3d] | Loss: %.3f | Acc: %.3f%% (%d/%d)'%(\n",
    "                    batch_idx+1, len(testloader), test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    scheduler.step()\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print('Current learning rate is {:f}'.format(param_group['lr']))\n",
    "                \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wo-ijIr5952c"
   },
   "source": [
    "## 5. Plot confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TztY64oz1_jJ"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),horizontalalignment=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "net.eval()\n",
    "ylabel = []\n",
    "yhatlabel = []\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    outputs = net(inputs)\n",
    "    _, predicted = outputs.max(1)\n",
    "    ylabel = np.concatenate((ylabel, targets.cpu().numpy()))\n",
    "    yhatlabel = np.concatenate((yhatlabel, predicted.cpu().numpy()))\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = skl.confusion_matrix(ylabel, yhatlabel)\n",
    "np.set_printoptions(precision=2)\n",
    "is_correct = (ylabel == yhatlabel)\n",
    "acc = np.sum(is_correct * 1) / len(is_correct)\n",
    "print('accuracy:%.5f' %acc)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes,\n",
    "                  title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,\n",
    "                  title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "5_5_2_Torch_trainCifar10_ResNet50.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
