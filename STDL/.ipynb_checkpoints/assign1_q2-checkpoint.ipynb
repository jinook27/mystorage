{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from math import sin\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# example of generating random samples from sin(x)\n",
    "from numpy.random import rand\n",
    "from numpy import hstack\n",
    "\n",
    "# define and fit a discriminator model\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import hstack\n",
    "from numpy.random import randn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select a One-Dimensional Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function\n",
    "def calculate(x):\n",
    "\treturn x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# demonstrate simple x^2 function\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# simple function\n",
    "def calculate(x):\n",
    "\treturn x * x\n",
    " \n",
    "# define inputs\n",
    "inputs = [-0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# calculate outputs\n",
    "outputs = [calculate(x) for x in inputs]\n",
    "# plot the result\n",
    "pyplot.plot(inputs, outputs)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of generating random samples from X^2\n",
    "from numpy.random import rand\n",
    "from numpy import hstack\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate randoms sample from x^2\n",
    "def generate_samples(n=100):\n",
    "\t# generate random inputs in [-0.5, 0.5]\n",
    "\tX1 = rand(n) - 0.5\n",
    "\t# generate outputs X^2 (quadratic)\n",
    "\tX2 = X1 * X1\n",
    "\t# stack arrays\n",
    "\tX1 = X1.reshape(n, 1)\n",
    "\tX2 = X2.reshape(n, 1)\n",
    "\treturn hstack((X1, X2))\n",
    "\n",
    "# generate samples\n",
    "data = generate_samples()\n",
    "# plot samples\n",
    "pyplot.scatter(data[:, 0], data[:, 1])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4375 0.734375\n",
      "1 0.265625 0.671875\n",
      "2 0.421875 0.671875\n",
      "3 0.390625 0.6875\n",
      "4 0.375 0.703125\n",
      "5 0.359375 0.75\n",
      "6 0.390625 0.734375\n",
      "7 0.359375 0.671875\n",
      "8 0.234375 0.734375\n",
      "9 0.40625 0.703125\n",
      "10 0.4375 0.78125\n",
      "11 0.3125 0.6875\n",
      "12 0.375 0.75\n",
      "13 0.3125 0.671875\n",
      "14 0.46875 0.6875\n",
      "15 0.40625 0.75\n",
      "16 0.484375 0.703125\n",
      "17 0.28125 0.671875\n",
      "18 0.4375 0.734375\n",
      "19 0.375 0.71875\n",
      "20 0.40625 0.6875\n",
      "21 0.453125 0.765625\n",
      "22 0.40625 0.6875\n",
      "23 0.375 0.59375\n",
      "24 0.375 0.75\n",
      "25 0.375 0.78125\n",
      "26 0.375 0.75\n",
      "27 0.328125 0.765625\n",
      "28 0.390625 0.765625\n",
      "29 0.359375 0.71875\n",
      "30 0.453125 0.78125\n",
      "31 0.40625 0.75\n",
      "32 0.328125 0.671875\n",
      "33 0.453125 0.71875\n",
      "34 0.453125 0.84375\n",
      "35 0.515625 0.734375\n",
      "36 0.421875 0.765625\n",
      "37 0.328125 0.75\n",
      "38 0.390625 0.640625\n",
      "39 0.34375 0.765625\n",
      "40 0.390625 0.6875\n",
      "41 0.46875 0.65625\n",
      "42 0.34375 0.78125\n",
      "43 0.40625 0.8125\n",
      "44 0.421875 0.734375\n",
      "45 0.4375 0.703125\n",
      "46 0.359375 0.75\n",
      "47 0.359375 0.734375\n",
      "48 0.4375 0.78125\n",
      "49 0.515625 0.6875\n",
      "50 0.4375 0.703125\n",
      "51 0.34375 0.671875\n",
      "52 0.28125 0.796875\n",
      "53 0.4375 0.78125\n",
      "54 0.421875 0.8125\n",
      "55 0.296875 0.75\n",
      "56 0.484375 0.8125\n",
      "57 0.46875 0.6875\n",
      "58 0.4375 0.75\n",
      "59 0.40625 0.703125\n",
      "60 0.46875 0.90625\n",
      "61 0.453125 0.75\n",
      "62 0.390625 0.84375\n",
      "63 0.40625 0.671875\n",
      "64 0.453125 0.734375\n",
      "65 0.390625 0.765625\n",
      "66 0.390625 0.8125\n",
      "67 0.4375 0.796875\n",
      "68 0.40625 0.734375\n",
      "69 0.40625 0.6875\n",
      "70 0.421875 0.8125\n",
      "71 0.359375 0.71875\n",
      "72 0.4375 0.875\n",
      "73 0.515625 0.65625\n",
      "74 0.375 0.765625\n",
      "75 0.515625 0.84375\n",
      "76 0.484375 0.8125\n",
      "77 0.390625 0.859375\n",
      "78 0.46875 0.75\n",
      "79 0.4375 0.796875\n",
      "80 0.40625 0.84375\n",
      "81 0.46875 0.65625\n",
      "82 0.484375 0.78125\n",
      "83 0.546875 0.8125\n",
      "84 0.375 0.8125\n",
      "85 0.4375 0.8125\n",
      "86 0.421875 0.828125\n",
      "87 0.4375 0.78125\n",
      "88 0.390625 0.75\n",
      "89 0.40625 0.71875\n",
      "90 0.390625 0.8125\n",
      "91 0.578125 0.859375\n",
      "92 0.5 0.890625\n",
      "93 0.515625 0.84375\n",
      "94 0.453125 0.875\n",
      "95 0.515625 0.90625\n",
      "96 0.375 0.8125\n",
      "97 0.421875 0.875\n",
      "98 0.515625 0.75\n",
      "99 0.5 0.796875\n",
      "100 0.5 0.796875\n",
      "101 0.515625 0.828125\n",
      "102 0.484375 0.765625\n",
      "103 0.390625 0.796875\n",
      "104 0.46875 0.84375\n",
      "105 0.515625 0.796875\n",
      "106 0.453125 0.921875\n",
      "107 0.5 0.828125\n",
      "108 0.5 0.796875\n",
      "109 0.46875 0.875\n",
      "110 0.5 0.78125\n",
      "111 0.484375 0.859375\n",
      "112 0.609375 0.78125\n",
      "113 0.5625 0.859375\n",
      "114 0.453125 0.84375\n",
      "115 0.546875 0.859375\n",
      "116 0.5625 0.859375\n",
      "117 0.5 0.828125\n",
      "118 0.484375 0.90625\n",
      "119 0.640625 0.890625\n",
      "120 0.53125 0.859375\n",
      "121 0.640625 0.84375\n",
      "122 0.625 0.921875\n",
      "123 0.5 0.90625\n",
      "124 0.5 0.84375\n",
      "125 0.546875 0.84375\n",
      "126 0.53125 0.78125\n",
      "127 0.65625 0.796875\n",
      "128 0.5625 0.84375\n",
      "129 0.46875 0.890625\n",
      "130 0.5625 0.796875\n",
      "131 0.484375 0.921875\n",
      "132 0.53125 0.921875\n",
      "133 0.5 0.890625\n",
      "134 0.6875 0.84375\n",
      "135 0.640625 0.875\n",
      "136 0.3125 0.890625\n",
      "137 0.640625 0.90625\n",
      "138 0.453125 0.953125\n",
      "139 0.640625 0.890625\n",
      "140 0.546875 0.859375\n",
      "141 0.515625 0.875\n",
      "142 0.5625 0.8125\n",
      "143 0.671875 0.84375\n",
      "144 0.515625 0.953125\n",
      "145 0.59375 0.9375\n",
      "146 0.59375 0.96875\n",
      "147 0.640625 0.875\n",
      "148 0.515625 0.875\n",
      "149 0.6875 0.828125\n",
      "150 0.703125 0.890625\n",
      "151 0.5 0.921875\n",
      "152 0.671875 0.984375\n",
      "153 0.515625 0.859375\n",
      "154 0.5625 0.890625\n",
      "155 0.53125 0.921875\n",
      "156 0.5625 0.859375\n",
      "157 0.609375 0.9375\n",
      "158 0.578125 0.875\n",
      "159 0.734375 0.953125\n",
      "160 0.65625 0.890625\n",
      "161 0.46875 0.921875\n",
      "162 0.65625 0.9375\n",
      "163 0.703125 0.921875\n",
      "164 0.5625 0.90625\n",
      "165 0.609375 0.921875\n",
      "166 0.59375 0.828125\n",
      "167 0.6875 0.90625\n",
      "168 0.625 0.890625\n",
      "169 0.578125 0.9375\n",
      "170 0.625 0.96875\n",
      "171 0.546875 0.953125\n",
      "172 0.671875 0.90625\n",
      "173 0.53125 0.921875\n",
      "174 0.65625 0.96875\n",
      "175 0.546875 0.953125\n",
      "176 0.6875 0.9375\n",
      "177 0.53125 0.921875\n",
      "178 0.640625 0.953125\n",
      "179 0.609375 0.9375\n",
      "180 0.640625 0.921875\n",
      "181 0.609375 0.9375\n",
      "182 0.703125 0.90625\n",
      "183 0.53125 0.953125\n",
      "184 0.6875 0.953125\n",
      "185 0.625 0.921875\n",
      "186 0.703125 0.921875\n",
      "187 0.703125 0.9375\n",
      "188 0.71875 1.0\n",
      "189 0.703125 0.953125\n",
      "190 0.609375 0.90625\n",
      "191 0.671875 0.890625\n",
      "192 0.6875 0.890625\n",
      "193 0.625 0.96875\n",
      "194 0.671875 0.921875\n",
      "195 0.59375 0.953125\n",
      "196 0.734375 0.9375\n",
      "197 0.703125 0.984375\n",
      "198 0.765625 0.9375\n",
      "199 0.65625 0.953125\n",
      "200 0.640625 0.96875\n",
      "201 0.640625 0.875\n",
      "202 0.71875 0.96875\n",
      "203 0.578125 0.9375\n",
      "204 0.671875 0.953125\n",
      "205 0.625 0.921875\n",
      "206 0.671875 0.9375\n",
      "207 0.640625 0.9375\n",
      "208 0.71875 0.9375\n",
      "209 0.796875 0.859375\n",
      "210 0.625 0.9375\n",
      "211 0.703125 0.984375\n",
      "212 0.671875 0.921875\n",
      "213 0.734375 0.96875\n",
      "214 0.734375 0.953125\n",
      "215 0.78125 0.890625\n",
      "216 0.703125 0.9375\n",
      "217 0.71875 0.953125\n",
      "218 0.75 0.96875\n",
      "219 0.703125 0.921875\n",
      "220 0.84375 0.953125\n",
      "221 0.78125 0.9375\n",
      "222 0.796875 0.953125\n",
      "223 0.734375 0.96875\n",
      "224 0.71875 0.984375\n",
      "225 0.765625 0.96875\n",
      "226 0.75 0.96875\n",
      "227 0.6875 0.953125\n",
      "228 0.8125 0.921875\n",
      "229 0.75 0.90625\n",
      "230 0.8125 0.953125\n",
      "231 0.78125 0.96875\n",
      "232 0.78125 0.96875\n",
      "233 0.75 0.9375\n",
      "234 0.8125 0.953125\n",
      "235 0.84375 0.9375\n",
      "236 0.8125 0.859375\n",
      "237 0.875 0.875\n",
      "238 0.796875 0.96875\n",
      "239 0.890625 0.984375\n",
      "240 0.671875 0.953125\n",
      "241 0.828125 0.921875\n",
      "242 0.8125 0.953125\n",
      "243 0.78125 0.921875\n",
      "244 0.8125 0.96875\n",
      "245 0.84375 0.90625\n",
      "246 0.78125 0.9375\n",
      "247 0.84375 0.953125\n",
      "248 0.734375 0.9375\n",
      "249 0.765625 0.953125\n",
      "250 0.859375 0.9375\n",
      "251 0.6875 0.875\n",
      "252 0.890625 0.921875\n",
      "253 0.78125 0.953125\n",
      "254 0.78125 0.96875\n",
      "255 0.875 0.859375\n",
      "256 0.828125 0.921875\n",
      "257 0.8125 0.953125\n",
      "258 0.921875 0.9375\n",
      "259 0.859375 0.90625\n",
      "260 0.828125 0.890625\n",
      "261 0.75 0.96875\n",
      "262 0.921875 0.953125\n",
      "263 0.84375 0.953125\n",
      "264 0.90625 0.9375\n",
      "265 0.90625 0.96875\n",
      "266 0.78125 0.96875\n",
      "267 0.84375 0.921875\n",
      "268 0.859375 0.9375\n",
      "269 0.921875 0.90625\n",
      "270 0.875 0.921875\n",
      "271 0.890625 0.9375\n",
      "272 0.890625 0.9375\n",
      "273 0.859375 0.96875\n",
      "274 0.875 0.953125\n",
      "275 0.875 0.921875\n",
      "276 0.96875 0.90625\n",
      "277 0.875 0.890625\n",
      "278 0.859375 0.96875\n",
      "279 0.859375 0.96875\n",
      "280 0.890625 0.90625\n",
      "281 0.953125 0.9375\n",
      "282 0.765625 0.921875\n",
      "283 0.84375 0.921875\n",
      "284 0.953125 0.921875\n",
      "285 0.796875 0.90625\n",
      "286 0.984375 0.9375\n",
      "287 0.859375 0.890625\n",
      "288 0.828125 0.953125\n",
      "289 0.84375 0.875\n",
      "290 0.84375 0.9375\n",
      "291 0.875 0.9375\n",
      "292 0.78125 0.90625\n",
      "293 0.828125 0.890625\n",
      "294 0.828125 0.984375\n",
      "295 0.8125 0.9375\n",
      "296 0.90625 0.9375\n",
      "297 0.84375 0.859375\n",
      "298 0.875 0.96875\n",
      "299 0.875 0.90625\n",
      "300 0.84375 0.890625\n",
      "301 0.953125 0.875\n",
      "302 0.890625 0.90625\n",
      "303 0.890625 0.953125\n",
      "304 0.890625 0.921875\n",
      "305 0.921875 0.9375\n",
      "306 0.890625 0.953125\n",
      "307 0.9375 0.90625\n",
      "308 0.859375 0.953125\n",
      "309 0.875 0.90625\n",
      "310 0.890625 0.921875\n",
      "311 0.96875 0.953125\n",
      "312 0.890625 0.921875\n",
      "313 0.84375 0.828125\n",
      "314 0.90625 0.90625\n",
      "315 0.921875 0.890625\n",
      "316 0.875 0.90625\n",
      "317 0.921875 0.9375\n",
      "318 0.921875 0.875\n",
      "319 0.921875 0.890625\n",
      "320 0.921875 0.859375\n",
      "321 0.9375 0.921875\n",
      "322 0.90625 0.921875\n",
      "323 0.890625 0.890625\n",
      "324 0.9375 0.953125\n",
      "325 0.90625 0.921875\n",
      "326 0.90625 0.984375\n",
      "327 0.921875 0.921875\n",
      "328 0.890625 0.953125\n",
      "329 0.9375 0.84375\n",
      "330 0.953125 0.953125\n",
      "331 0.9375 0.96875\n",
      "332 0.90625 0.90625\n",
      "333 0.984375 0.875\n",
      "334 0.921875 0.875\n",
      "335 0.9375 0.921875\n",
      "336 0.921875 0.890625\n",
      "337 0.921875 0.921875\n",
      "338 0.890625 0.921875\n",
      "339 0.9375 0.796875\n",
      "340 0.890625 0.90625\n",
      "341 0.875 0.890625\n",
      "342 0.921875 0.9375\n",
      "343 0.921875 0.875\n",
      "344 0.953125 0.90625\n",
      "345 0.859375 0.9375\n",
      "346 0.96875 0.90625\n",
      "347 0.90625 0.96875\n",
      "348 0.9375 0.9375\n",
      "349 0.90625 0.890625\n",
      "350 0.953125 0.9375\n",
      "351 0.9375 0.859375\n",
      "352 0.921875 0.96875\n",
      "353 0.953125 0.90625\n",
      "354 0.890625 0.828125\n",
      "355 0.96875 0.9375\n",
      "356 0.890625 0.90625\n",
      "357 0.953125 0.921875\n",
      "358 0.90625 0.953125\n",
      "359 0.9375 0.90625\n",
      "360 0.890625 0.90625\n",
      "361 0.953125 0.921875\n",
      "362 0.90625 0.9375\n",
      "363 0.9375 0.90625\n",
      "364 0.96875 0.953125\n",
      "365 0.96875 0.90625\n",
      "366 0.984375 0.921875\n",
      "367 0.96875 0.90625\n",
      "368 0.921875 0.921875\n",
      "369 0.96875 0.953125\n",
      "370 0.90625 0.921875\n",
      "371 0.90625 0.859375\n",
      "372 0.953125 0.921875\n",
      "373 0.921875 0.921875\n",
      "374 0.96875 0.90625\n",
      "375 0.9375 0.921875\n",
      "376 0.953125 0.890625\n",
      "377 0.9375 0.9375\n",
      "378 0.921875 0.875\n",
      "379 0.953125 0.984375\n",
      "380 0.921875 0.875\n",
      "381 0.984375 0.90625\n",
      "382 1.0 0.90625\n",
      "383 0.96875 0.90625\n",
      "384 0.96875 0.84375\n",
      "385 0.984375 0.921875\n",
      "386 0.96875 0.984375\n",
      "387 0.953125 0.84375\n",
      "388 0.984375 0.890625\n",
      "389 1.0 0.96875\n",
      "390 0.984375 0.828125\n",
      "391 0.953125 0.90625\n",
      "392 0.984375 0.921875\n",
      "393 0.96875 0.921875\n",
      "394 0.984375 0.859375\n",
      "395 0.921875 0.828125\n",
      "396 0.921875 0.84375\n",
      "397 0.9375 0.859375\n",
      "398 0.953125 0.890625\n",
      "399 0.953125 0.90625\n",
      "400 0.96875 0.9375\n",
      "401 0.984375 0.921875\n",
      "402 0.96875 0.828125\n",
      "403 1.0 0.890625\n",
      "404 0.890625 0.90625\n",
      "405 0.984375 0.796875\n",
      "406 0.984375 0.890625\n",
      "407 0.953125 0.921875\n",
      "408 0.96875 0.875\n",
      "409 0.90625 0.875\n",
      "410 0.96875 0.890625\n",
      "411 0.953125 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412 0.96875 0.9375\n",
      "413 0.9375 0.9375\n",
      "414 1.0 0.9375\n",
      "415 0.96875 0.953125\n",
      "416 0.984375 0.921875\n",
      "417 0.953125 0.9375\n",
      "418 0.953125 0.90625\n",
      "419 0.9375 0.796875\n",
      "420 0.96875 0.96875\n",
      "421 0.96875 0.890625\n",
      "422 0.96875 0.890625\n",
      "423 1.0 0.96875\n",
      "424 0.953125 0.875\n",
      "425 0.9375 0.84375\n",
      "426 0.921875 0.890625\n",
      "427 0.90625 0.921875\n",
      "428 0.96875 0.84375\n",
      "429 0.984375 0.859375\n",
      "430 0.96875 0.859375\n",
      "431 0.953125 0.90625\n",
      "432 0.9375 0.921875\n",
      "433 0.984375 0.90625\n",
      "434 0.984375 0.828125\n",
      "435 0.90625 0.84375\n",
      "436 1.0 0.859375\n",
      "437 0.96875 0.890625\n",
      "438 0.96875 0.84375\n",
      "439 1.0 0.8125\n",
      "440 0.9375 0.875\n",
      "441 0.96875 0.90625\n",
      "442 0.953125 0.9375\n",
      "443 0.96875 0.890625\n",
      "444 0.953125 0.859375\n",
      "445 0.96875 0.90625\n",
      "446 0.9375 0.890625\n",
      "447 1.0 0.84375\n",
      "448 0.96875 0.890625\n",
      "449 0.96875 0.859375\n",
      "450 0.984375 0.90625\n",
      "451 1.0 0.828125\n",
      "452 0.953125 0.875\n",
      "453 0.96875 0.84375\n",
      "454 0.984375 0.8125\n",
      "455 0.953125 0.9375\n",
      "456 1.0 0.8125\n",
      "457 0.984375 0.890625\n",
      "458 0.984375 0.953125\n",
      "459 0.984375 0.953125\n",
      "460 0.953125 0.921875\n",
      "461 0.984375 0.9375\n",
      "462 0.984375 0.875\n",
      "463 0.984375 0.859375\n",
      "464 0.984375 0.96875\n",
      "465 0.953125 0.921875\n",
      "466 1.0 0.9375\n",
      "467 1.0 0.90625\n",
      "468 0.96875 0.859375\n",
      "469 0.96875 0.890625\n",
      "470 0.984375 0.828125\n",
      "471 0.984375 0.796875\n",
      "472 1.0 0.890625\n",
      "473 1.0 0.890625\n",
      "474 0.984375 0.90625\n",
      "475 0.96875 0.828125\n",
      "476 0.984375 0.859375\n",
      "477 0.984375 0.96875\n",
      "478 0.953125 0.921875\n",
      "479 0.96875 0.890625\n",
      "480 1.0 0.828125\n",
      "481 0.96875 0.859375\n",
      "482 0.984375 0.875\n",
      "483 1.0 0.859375\n",
      "484 0.984375 0.828125\n",
      "485 0.984375 0.796875\n",
      "486 0.984375 0.921875\n",
      "487 0.96875 0.8125\n",
      "488 0.984375 0.890625\n",
      "489 0.984375 0.75\n",
      "490 0.984375 0.875\n",
      "491 0.984375 0.90625\n",
      "492 1.0 0.90625\n",
      "493 0.984375 0.875\n",
      "494 0.96875 0.875\n",
      "495 0.953125 0.9375\n",
      "496 0.984375 0.796875\n",
      "497 0.984375 0.84375\n",
      "498 0.96875 0.9375\n",
      "499 1.0 0.828125\n",
      "500 1.0 0.828125\n",
      "501 1.0 0.890625\n",
      "502 0.984375 0.890625\n",
      "503 0.984375 0.828125\n",
      "504 0.984375 0.875\n",
      "505 1.0 0.875\n",
      "506 1.0 0.890625\n",
      "507 1.0 0.90625\n",
      "508 0.984375 0.921875\n",
      "509 0.984375 0.9375\n",
      "510 0.984375 0.9375\n",
      "511 0.984375 0.828125\n",
      "512 1.0 0.859375\n",
      "513 0.984375 0.90625\n",
      "514 0.984375 0.90625\n",
      "515 1.0 0.859375\n",
      "516 0.96875 0.921875\n",
      "517 1.0 0.90625\n",
      "518 1.0 0.84375\n",
      "519 1.0 0.84375\n",
      "520 1.0 0.890625\n",
      "521 0.984375 0.859375\n",
      "522 1.0 0.90625\n",
      "523 0.984375 0.859375\n",
      "524 0.984375 0.875\n",
      "525 0.984375 0.890625\n",
      "526 1.0 0.859375\n",
      "527 0.984375 0.953125\n",
      "528 0.9375 0.828125\n",
      "529 0.953125 0.890625\n",
      "530 1.0 0.921875\n",
      "531 0.984375 0.8125\n",
      "532 0.984375 0.921875\n",
      "533 1.0 0.875\n",
      "534 0.984375 0.890625\n",
      "535 1.0 0.9375\n",
      "536 0.984375 0.84375\n",
      "537 0.984375 0.90625\n",
      "538 0.984375 0.78125\n",
      "539 0.984375 0.9375\n",
      "540 0.984375 0.84375\n",
      "541 0.984375 0.90625\n",
      "542 1.0 0.90625\n",
      "543 1.0 0.96875\n",
      "544 1.0 0.90625\n",
      "545 1.0 0.890625\n",
      "546 1.0 0.890625\n",
      "547 1.0 0.90625\n",
      "548 1.0 0.90625\n",
      "549 1.0 0.875\n",
      "550 1.0 0.796875\n",
      "551 1.0 0.859375\n",
      "552 1.0 0.921875\n",
      "553 1.0 0.875\n",
      "554 1.0 0.859375\n",
      "555 1.0 0.921875\n",
      "556 1.0 0.8125\n",
      "557 1.0 0.921875\n",
      "558 1.0 0.859375\n",
      "559 1.0 0.859375\n",
      "560 0.984375 0.875\n",
      "561 1.0 0.765625\n",
      "562 1.0 0.9375\n",
      "563 1.0 0.90625\n",
      "564 1.0 0.84375\n",
      "565 1.0 0.875\n",
      "566 1.0 0.859375\n",
      "567 1.0 0.890625\n",
      "568 1.0 0.90625\n",
      "569 1.0 0.84375\n",
      "570 1.0 0.8125\n",
      "571 1.0 0.9375\n",
      "572 1.0 0.84375\n",
      "573 1.0 0.875\n",
      "574 1.0 0.890625\n",
      "575 1.0 0.859375\n",
      "576 1.0 0.859375\n",
      "577 1.0 0.90625\n",
      "578 1.0 0.859375\n",
      "579 1.0 0.84375\n",
      "580 1.0 0.875\n",
      "581 1.0 0.90625\n",
      "582 1.0 0.796875\n",
      "583 1.0 0.828125\n",
      "584 1.0 0.90625\n",
      "585 1.0 0.828125\n",
      "586 1.0 0.859375\n",
      "587 1.0 0.890625\n",
      "588 1.0 0.921875\n",
      "589 1.0 0.8125\n",
      "590 1.0 0.859375\n",
      "591 1.0 0.9375\n",
      "592 1.0 0.9375\n",
      "593 1.0 0.90625\n",
      "594 1.0 0.828125\n",
      "595 1.0 0.875\n",
      "596 1.0 0.890625\n",
      "597 1.0 0.859375\n",
      "598 1.0 0.9375\n",
      "599 1.0 0.796875\n",
      "600 1.0 0.828125\n",
      "601 1.0 0.890625\n",
      "602 1.0 0.875\n",
      "603 1.0 0.875\n",
      "604 1.0 0.9375\n",
      "605 1.0 0.921875\n",
      "606 1.0 0.875\n",
      "607 1.0 0.8125\n",
      "608 1.0 0.890625\n",
      "609 1.0 0.890625\n",
      "610 1.0 0.84375\n",
      "611 1.0 0.953125\n",
      "612 1.0 0.828125\n",
      "613 1.0 0.828125\n",
      "614 1.0 0.859375\n",
      "615 1.0 0.859375\n",
      "616 1.0 0.90625\n",
      "617 1.0 0.890625\n",
      "618 1.0 0.890625\n",
      "619 1.0 0.84375\n",
      "620 1.0 0.9375\n",
      "621 1.0 0.765625\n",
      "622 1.0 0.828125\n",
      "623 1.0 0.828125\n",
      "624 1.0 0.8125\n",
      "625 1.0 0.84375\n",
      "626 1.0 0.84375\n",
      "627 1.0 0.90625\n",
      "628 1.0 0.875\n",
      "629 1.0 0.859375\n",
      "630 1.0 0.8125\n",
      "631 1.0 0.84375\n",
      "632 1.0 0.90625\n",
      "633 1.0 0.8125\n",
      "634 1.0 0.921875\n",
      "635 1.0 0.890625\n",
      "636 1.0 0.828125\n",
      "637 1.0 0.90625\n",
      "638 1.0 0.90625\n",
      "639 1.0 0.859375\n",
      "640 1.0 0.953125\n",
      "641 1.0 0.8125\n",
      "642 1.0 0.828125\n",
      "643 1.0 0.84375\n",
      "644 1.0 0.796875\n",
      "645 1.0 0.859375\n",
      "646 1.0 0.90625\n",
      "647 1.0 0.859375\n",
      "648 1.0 0.875\n",
      "649 1.0 0.921875\n",
      "650 1.0 0.921875\n",
      "651 1.0 0.890625\n",
      "652 1.0 0.921875\n",
      "653 1.0 0.890625\n",
      "654 1.0 0.9375\n",
      "655 1.0 0.796875\n",
      "656 1.0 0.921875\n",
      "657 1.0 0.90625\n",
      "658 1.0 0.921875\n",
      "659 1.0 0.796875\n",
      "660 1.0 0.890625\n",
      "661 1.0 0.875\n",
      "662 1.0 0.890625\n",
      "663 1.0 0.875\n",
      "664 1.0 0.90625\n",
      "665 1.0 0.8125\n",
      "666 1.0 0.890625\n",
      "667 1.0 0.859375\n",
      "668 1.0 0.8125\n",
      "669 1.0 0.78125\n",
      "670 1.0 0.859375\n",
      "671 1.0 0.828125\n",
      "672 1.0 0.890625\n",
      "673 1.0 0.9375\n",
      "674 1.0 0.90625\n",
      "675 1.0 0.875\n",
      "676 1.0 0.875\n",
      "677 1.0 0.859375\n",
      "678 1.0 0.875\n",
      "679 1.0 0.796875\n",
      "680 1.0 0.78125\n",
      "681 1.0 0.921875\n",
      "682 1.0 0.859375\n",
      "683 1.0 0.890625\n",
      "684 1.0 0.90625\n",
      "685 1.0 0.90625\n",
      "686 1.0 0.8125\n",
      "687 1.0 0.859375\n",
      "688 1.0 0.90625\n",
      "689 1.0 0.875\n",
      "690 1.0 0.921875\n",
      "691 1.0 0.953125\n",
      "692 1.0 0.875\n",
      "693 1.0 0.75\n",
      "694 1.0 0.84375\n",
      "695 1.0 0.828125\n",
      "696 1.0 0.875\n",
      "697 1.0 0.796875\n",
      "698 1.0 0.859375\n",
      "699 1.0 0.90625\n",
      "700 1.0 0.875\n",
      "701 1.0 0.828125\n",
      "702 1.0 0.9375\n",
      "703 1.0 0.859375\n",
      "704 1.0 0.765625\n",
      "705 1.0 0.828125\n",
      "706 1.0 0.828125\n",
      "707 1.0 0.890625\n",
      "708 1.0 0.953125\n",
      "709 1.0 0.890625\n",
      "710 1.0 0.90625\n",
      "711 1.0 0.859375\n",
      "712 1.0 0.796875\n",
      "713 1.0 0.9375\n",
      "714 1.0 0.890625\n",
      "715 1.0 0.78125\n",
      "716 1.0 0.875\n",
      "717 1.0 0.890625\n",
      "718 1.0 0.90625\n",
      "719 1.0 0.875\n",
      "720 1.0 0.890625\n",
      "721 1.0 0.921875\n",
      "722 1.0 0.90625\n",
      "723 1.0 0.90625\n",
      "724 1.0 0.921875\n",
      "725 1.0 0.8125\n",
      "726 1.0 0.921875\n",
      "727 1.0 0.84375\n",
      "728 1.0 0.796875\n",
      "729 1.0 0.875\n",
      "730 1.0 0.859375\n",
      "731 1.0 0.90625\n",
      "732 1.0 0.875\n",
      "733 1.0 0.859375\n",
      "734 1.0 0.859375\n",
      "735 1.0 0.90625\n",
      "736 1.0 0.859375\n",
      "737 1.0 0.796875\n",
      "738 1.0 0.875\n",
      "739 1.0 0.890625\n",
      "740 1.0 0.890625\n",
      "741 1.0 0.921875\n",
      "742 1.0 0.890625\n",
      "743 1.0 0.90625\n",
      "744 1.0 0.875\n",
      "745 1.0 0.859375\n",
      "746 1.0 0.921875\n",
      "747 1.0 0.84375\n",
      "748 1.0 0.859375\n",
      "749 1.0 0.890625\n",
      "750 1.0 0.875\n",
      "751 1.0 0.921875\n",
      "752 1.0 0.890625\n",
      "753 1.0 0.890625\n",
      "754 1.0 0.96875\n",
      "755 1.0 0.921875\n",
      "756 1.0 0.890625\n",
      "757 1.0 0.890625\n",
      "758 1.0 0.890625\n",
      "759 1.0 0.890625\n",
      "760 1.0 0.828125\n",
      "761 1.0 0.875\n",
      "762 1.0 0.84375\n",
      "763 1.0 0.90625\n",
      "764 1.0 0.890625\n",
      "765 1.0 0.90625\n",
      "766 1.0 0.859375\n",
      "767 1.0 0.796875\n",
      "768 1.0 0.828125\n",
      "769 1.0 0.828125\n",
      "770 1.0 0.90625\n",
      "771 1.0 0.828125\n",
      "772 1.0 0.875\n",
      "773 1.0 0.890625\n",
      "774 1.0 0.90625\n",
      "775 1.0 0.90625\n",
      "776 1.0 0.90625\n",
      "777 1.0 0.84375\n",
      "778 1.0 0.875\n",
      "779 1.0 0.953125\n",
      "780 1.0 0.890625\n",
      "781 1.0 0.890625\n",
      "782 1.0 0.828125\n",
      "783 1.0 0.9375\n",
      "784 1.0 0.890625\n",
      "785 1.0 0.9375\n",
      "786 1.0 0.921875\n",
      "787 1.0 0.875\n",
      "788 1.0 0.8125\n",
      "789 1.0 0.921875\n",
      "790 1.0 0.796875\n",
      "791 1.0 0.84375\n",
      "792 1.0 0.890625\n",
      "793 1.0 0.8125\n",
      "794 1.0 0.84375\n",
      "795 1.0 0.90625\n",
      "796 1.0 0.921875\n",
      "797 1.0 0.875\n",
      "798 1.0 0.890625\n",
      "799 1.0 0.859375\n",
      "800 1.0 0.859375\n",
      "801 1.0 0.84375\n",
      "802 1.0 0.921875\n",
      "803 1.0 0.90625\n",
      "804 1.0 0.875\n",
      "805 1.0 0.859375\n",
      "806 1.0 0.859375\n",
      "807 1.0 0.890625\n",
      "808 1.0 0.828125\n",
      "809 1.0 0.890625\n",
      "810 1.0 0.875\n",
      "811 1.0 0.8125\n",
      "812 1.0 0.796875\n",
      "813 1.0 0.828125\n",
      "814 1.0 0.859375\n",
      "815 1.0 0.8125\n",
      "816 1.0 0.890625\n",
      "817 1.0 0.84375\n",
      "818 1.0 0.828125\n",
      "819 1.0 0.953125\n",
      "820 1.0 0.890625\n",
      "821 1.0 0.765625\n",
      "822 1.0 0.8125\n",
      "823 1.0 0.796875\n",
      "824 1.0 0.828125\n",
      "825 1.0 0.875\n",
      "826 1.0 0.9375\n",
      "827 1.0 0.859375\n",
      "828 1.0 0.890625\n",
      "829 1.0 0.84375\n",
      "830 1.0 0.875\n",
      "831 1.0 0.828125\n",
      "832 1.0 0.828125\n",
      "833 1.0 0.90625\n",
      "834 1.0 0.921875\n",
      "835 1.0 0.9375\n",
      "836 1.0 0.84375\n",
      "837 1.0 0.796875\n",
      "838 1.0 0.890625\n",
      "839 1.0 0.84375\n",
      "840 1.0 0.796875\n",
      "841 1.0 0.90625\n",
      "842 1.0 0.921875\n",
      "843 1.0 0.875\n",
      "844 1.0 0.875\n",
      "845 1.0 0.921875\n",
      "846 1.0 0.828125\n",
      "847 1.0 0.8125\n",
      "848 1.0 0.84375\n",
      "849 1.0 0.890625\n",
      "850 1.0 0.859375\n",
      "851 1.0 0.859375\n",
      "852 1.0 0.8125\n",
      "853 1.0 0.859375\n",
      "854 1.0 0.921875\n",
      "855 1.0 0.828125\n",
      "856 1.0 0.90625\n",
      "857 1.0 0.84375\n",
      "858 1.0 0.890625\n",
      "859 1.0 0.921875\n",
      "860 1.0 0.859375\n",
      "861 1.0 0.875\n",
      "862 1.0 0.90625\n",
      "863 1.0 0.71875\n",
      "864 1.0 0.84375\n",
      "865 1.0 0.875\n",
      "866 1.0 0.90625\n",
      "867 1.0 0.875\n",
      "868 1.0 0.890625\n",
      "869 1.0 0.84375\n",
      "870 1.0 0.859375\n",
      "871 1.0 0.890625\n",
      "872 1.0 0.921875\n",
      "873 1.0 0.890625\n",
      "874 1.0 0.890625\n",
      "875 1.0 0.90625\n",
      "876 1.0 0.9375\n",
      "877 1.0 0.890625\n",
      "878 1.0 0.90625\n",
      "879 1.0 0.90625\n",
      "880 1.0 0.859375\n",
      "881 1.0 0.90625\n",
      "882 1.0 0.859375\n",
      "883 1.0 0.875\n",
      "884 1.0 0.875\n",
      "885 1.0 0.890625\n",
      "886 1.0 0.859375\n",
      "887 1.0 0.796875\n",
      "888 1.0 0.921875\n",
      "889 1.0 0.859375\n",
      "890 1.0 0.90625\n",
      "891 1.0 0.828125\n",
      "892 1.0 0.859375\n",
      "893 1.0 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894 1.0 0.875\n",
      "895 1.0 0.875\n",
      "896 1.0 0.90625\n",
      "897 1.0 0.875\n",
      "898 1.0 0.859375\n",
      "899 1.0 0.890625\n",
      "900 1.0 0.859375\n",
      "901 1.0 0.796875\n",
      "902 1.0 0.921875\n",
      "903 1.0 0.859375\n",
      "904 1.0 0.859375\n",
      "905 1.0 0.953125\n",
      "906 1.0 0.75\n",
      "907 1.0 0.875\n",
      "908 1.0 0.859375\n",
      "909 1.0 0.875\n",
      "910 1.0 0.828125\n",
      "911 1.0 0.828125\n",
      "912 1.0 0.859375\n",
      "913 1.0 0.890625\n",
      "914 1.0 0.84375\n",
      "915 1.0 0.828125\n",
      "916 1.0 0.8125\n",
      "917 1.0 0.9375\n",
      "918 1.0 0.8125\n",
      "919 1.0 0.859375\n",
      "920 1.0 0.90625\n",
      "921 1.0 0.859375\n",
      "922 1.0 0.90625\n",
      "923 1.0 0.84375\n",
      "924 1.0 0.84375\n",
      "925 1.0 0.875\n",
      "926 1.0 0.90625\n",
      "927 1.0 0.90625\n",
      "928 1.0 0.921875\n",
      "929 1.0 0.90625\n",
      "930 1.0 0.859375\n",
      "931 1.0 0.859375\n",
      "932 1.0 0.859375\n",
      "933 1.0 0.875\n",
      "934 1.0 0.796875\n",
      "935 1.0 0.921875\n",
      "936 1.0 0.9375\n",
      "937 1.0 0.84375\n",
      "938 1.0 0.9375\n",
      "939 1.0 0.890625\n",
      "940 1.0 0.90625\n",
      "941 1.0 0.828125\n",
      "942 1.0 0.875\n",
      "943 1.0 0.78125\n",
      "944 1.0 0.859375\n",
      "945 1.0 0.90625\n",
      "946 1.0 0.890625\n",
      "947 1.0 0.9375\n",
      "948 1.0 0.953125\n",
      "949 1.0 0.859375\n",
      "950 1.0 0.875\n",
      "951 1.0 0.859375\n",
      "952 1.0 0.9375\n",
      "953 1.0 0.9375\n",
      "954 1.0 0.875\n",
      "955 1.0 0.859375\n",
      "956 1.0 0.859375\n",
      "957 1.0 0.921875\n",
      "958 1.0 0.8125\n",
      "959 1.0 0.84375\n",
      "960 1.0 0.84375\n",
      "961 1.0 0.875\n",
      "962 1.0 0.84375\n",
      "963 1.0 0.828125\n",
      "964 1.0 0.875\n",
      "965 1.0 0.859375\n",
      "966 1.0 0.8125\n",
      "967 1.0 0.859375\n",
      "968 1.0 0.984375\n",
      "969 1.0 0.84375\n",
      "970 1.0 0.921875\n",
      "971 1.0 0.90625\n",
      "972 1.0 0.84375\n",
      "973 1.0 0.90625\n",
      "974 1.0 0.90625\n",
      "975 1.0 0.921875\n",
      "976 1.0 0.90625\n",
      "977 1.0 0.859375\n",
      "978 1.0 0.90625\n",
      "979 1.0 0.953125\n",
      "980 1.0 0.875\n",
      "981 1.0 0.875\n",
      "982 1.0 0.859375\n",
      "983 1.0 0.890625\n",
      "984 1.0 0.921875\n",
      "985 1.0 0.75\n",
      "986 1.0 0.90625\n",
      "987 1.0 0.890625\n",
      "988 1.0 0.8125\n",
      "989 1.0 0.859375\n",
      "990 1.0 0.78125\n",
      "991 1.0 0.9375\n",
      "992 1.0 0.9375\n",
      "993 1.0 0.90625\n",
      "994 1.0 0.890625\n",
      "995 1.0 0.796875\n",
      "996 1.0 0.859375\n",
      "997 1.0 0.859375\n",
      "998 1.0 0.84375\n",
      "999 1.0 0.9375\n"
     ]
    }
   ],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# generate n real samples with class labels\n",
    "def generate_real_samples(n):\n",
    "\t# generate inputs in [-0.5, 0.5]\n",
    "\tX1 = rand(n) - 0.5\n",
    "\t# generate outputs X^2\n",
    "\tX2 = X1 * X1\n",
    "\t# stack arrays\n",
    "\tX1 = X1.reshape(n, 1)\n",
    "\tX2 = X2.reshape(n, 1)\n",
    "\tX = hstack((X1, X2))\n",
    "\t# generate class labels\n",
    "\ty = ones((n, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# generate n fake samples with class labels\n",
    "def generate_fake_samples(n):\n",
    "\t# generate inputs in [-1, 1]\n",
    "\tX1 = -1 + rand(n) * 2\n",
    "\t# generate outputs in [-1, 1]\n",
    "\tX2 = -1 + rand(n) * 2\n",
    "\t# stack arrays\n",
    "\tX1 = X1.reshape(n, 1)\n",
    "\tX2 = X2.reshape(n, 1)\n",
    "\tX = hstack((X1, X2))\n",
    "\t# generate class labels\n",
    "\ty = zeros((n, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# train the discriminator model\n",
    "def train_discriminator(model, n_epochs=1000, n_batch=128):\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# run epochs manually\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# generate real examples\n",
    "\t\tX_real, y_real = generate_real_samples(half_batch)\n",
    "\t\t# update model\n",
    "\t\tmodel.train_on_batch(X_real, y_real)\n",
    "\t\t# generate fake examples\n",
    "\t\tX_fake, y_fake = generate_fake_samples(half_batch)\n",
    "\t\t# update model\n",
    "\t\tmodel.train_on_batch(X_fake, y_fake)\n",
    "\t\t# evaluate the model\n",
    "\t\t_, acc_real = model.evaluate(X_real, y_real, verbose=0)\n",
    "\t\t_, acc_fake = model.evaluate(X_fake, y_fake, verbose=0)\n",
    "\t\tprint(i, acc_real, acc_fake)\n",
    "\n",
    "# define the discriminator model\n",
    "model = define_discriminator()\n",
    "# fit the model\n",
    "train_discriminator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "\tmodel.add(Dense(n_outputs, activation='linear'))\n",
    "\treturn model\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = randn(latent_dim * n)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "# use the generator to generate n fake examples and plot the results\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n)\n",
    "\t# predict outputs\n",
    "\tX = generator.predict(x_input)\n",
    "\t# plot the results\n",
    "\tpyplot.scatter(X[:, 0], X[:, 1])\n",
    "\tpyplot.show()\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 5\n",
    "# define the discriminator model\n",
    "model = define_generator(latent_dim)\n",
    "# generate and plot generated samples\n",
    "generate_fake_samples(model, latent_dim, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tdiscriminator.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(generator)\n",
    "\t# add the discriminator\n",
    "\tmodel.add(discriminator)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate creating the three models in the gan\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "\tmodel.add(Dense(n_outputs, activation='linear'))\n",
    "\treturn model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tdiscriminator.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(generator)\n",
    "\t# add the discriminator\n",
    "\tmodel.add(discriminator)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 5\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# summarize gan model\n",
    "gan_model.summary()\n",
    "# plot gan model\n",
    "#plot_model(gan_model, to_file='gan_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the composite model\n",
    "def train_gan(gan_model, latent_dim, n_epochs=10000, n_batch=128):\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# prepare points in latent space as input for the generator\n",
    "\t\tx_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t# create inverted labels for the fake samples\n",
    "\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\tgan_model.train_on_batch(x_gan, y_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128):\n",
    "\t# determine half the size of one batch, for updating the discriminator\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# prepare real samples\n",
    "\t\tx_real, y_real = generate_real_samples(half_batch)\n",
    "\t\t# prepare fake examples\n",
    "\t\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t# update discriminator\n",
    "\t\td_model.train_on_batch(x_real, y_real)\n",
    "\t\td_model.train_on_batch(x_fake, y_fake)\n",
    "\t\t# prepare points in latent space as input for the generator\n",
    "\t\tx_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t# create inverted labels for the fake samples\n",
    "\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\tgan_model.train_on_batch(x_gan, y_gan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Performance of the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n real samples with class labels\n",
    "def generate_real_samples(n):\n",
    "\t# generate inputs in [-0.5, 0.5]\n",
    "\tX1 = rand(n) - 0.5\n",
    "\t# generate outputs X^2\n",
    "\tX2 = X1 * X1\n",
    "\t# stack arrays\n",
    "\tX1 = X1.reshape(n, 1)\n",
    "\tX2 = X2.reshape(n, 1)\n",
    "\tX = hstack((X1, X2))\n",
    "\t# generate class labels\n",
    "\ty = ones((n, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = randn(latent_dim * n)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n, latent_dim)\n",
    "\treturn x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n)\n",
    "\t# predict outputs\n",
    "\tX = generator.predict(x_input)\n",
    "\t# create class labels\n",
    "\ty = zeros((n, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot real and fake points\n",
    "def summarize_performance(generator, latent_dim, n=100):\n",
    "\t# prepare real samples\n",
    "\tx_real, y_real = generate_real_samples(n)\n",
    "\t# prepare fake examples\n",
    "\tx_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "\t# scatter plot real and fake data points\n",
    "\tpyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    "\tpyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "\tpyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the discriminator and plot real and fake points\n",
    "def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):\n",
    "\t# prepare real samples\n",
    "\tx_real, y_real = generate_real_samples(n)\n",
    "\t# evaluate discriminator on real examples\n",
    "\t_, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "\t# prepare fake examples\n",
    "\tx_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "\t# evaluate discriminator on fake examples\n",
    "\t_, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "\t# summarize discriminator performance\n",
    "\tprint(epoch, acc_real, acc_fake)\n",
    "\t# scatter plot real and fake data points\n",
    "\tpyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    "\tpyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "\tpyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=2000):\n",
    "\t# determine half the size of one batch, for updating the discriminator\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# prepare real samples\n",
    "\t\tx_real, y_real = generate_real_samples(half_batch)\n",
    "\t\t# prepare fake examples\n",
    "\t\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t# update discriminator\n",
    "\t\td_model.train_on_batch(x_real, y_real)\n",
    "\t\td_model.train_on_batch(x_fake, y_fake)\n",
    "\t\t# prepare points in latent space as input for the generator\n",
    "\t\tx_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t# create inverted labels for the fake samples\n",
    "\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\tgan_model.train_on_batch(x_gan, y_gan)\n",
    "\t\t# evaluate the model every n_eval epochs\n",
    "\t\tif (i+1) % n_eval == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, d_model, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a generative adversarial network on a one-dimensional function\n",
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "\tmodel.add(Dense(n_outputs, activation='linear'))\n",
    "\treturn model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tdiscriminator.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(generator)\n",
    "\t# add the discriminator\n",
    "\tmodel.add(discriminator)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "# generate n real samples with class labels\n",
    "def generate_real_samples(n):\n",
    "\t# generate inputs in [-0.5, 0.5]\n",
    "\tX1 = rand(n) - 0.5\n",
    "\t# generate outputs X^2\n",
    "\tX2 = X1 * X1\n",
    "\t# stack arrays\n",
    "\tX1 = X1.reshape(n, 1)\n",
    "\tX2 = X2.reshape(n, 1)\n",
    "\tX = hstack((X1, X2))\n",
    "\t# generate class labels\n",
    "\ty = ones((n, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = randn(latent_dim * n)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n)\n",
    "\t# predict outputs\n",
    "\tX = generator.predict(x_input)\n",
    "\t# create class labels\n",
    "\ty = zeros((n, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# evaluate the discriminator and plot real and fake points\n",
    "def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):\n",
    "\t# prepare real samples\n",
    "\tx_real, y_real = generate_real_samples(n)\n",
    "\t# evaluate discriminator on real examples\n",
    "\t_, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "\t# prepare fake examples\n",
    "\tx_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "\t# evaluate discriminator on fake examples\n",
    "\t_, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "\t# summarize discriminator performance\n",
    "\tprint(epoch, acc_real, acc_fake)\n",
    "\t# scatter plot real and fake data points\n",
    "\tpyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    "\tpyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "\tpyplot.show()\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=2000):\n",
    "\t# determine half the size of one batch, for updating the discriminator\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# prepare real samples\n",
    "\t\tx_real, y_real = generate_real_samples(half_batch)\n",
    "\t\t# prepare fake examples\n",
    "\t\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t# update discriminator\n",
    "\t\td_model.train_on_batch(x_real, y_real)\n",
    "\t\td_model.train_on_batch(x_fake, y_fake)\n",
    "\t\t# prepare points in latent space as input for the generator\n",
    "\t\tx_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t# create inverted labels for the fake samples\n",
    "\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\tgan_model.train_on_batch(x_gan, y_gan)\n",
    "\t\t# evaluate the model every n_eval epochs\n",
    "\t\tif (i+1) % n_eval == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, d_model, latent_dim)\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 5\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
